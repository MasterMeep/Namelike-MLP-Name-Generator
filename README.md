Bigram.ipynb: "naive model" which mimics the inputs on a character level basis, i.e given a letter what comes next, uses simple bigram structure
multi-layer-perceptron.ipynb and deprecated ver: restructured version that uses torchlike methods of forward prop and uses autograd for back prop. unorganized and unscalable version. non deprecated ver is less structured new ver
torchified-MLP.ipynb: restructured the multi-layer-perceptron.ipynb into a scalable version with more torchlike implementations of classes for layers, implements autograd for backprop
